{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3da1338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Scraping page 1\n",
      "==> Scraping page 2\n",
      "==> Scraping page 3\n",
      "==> Scraping page 4\n",
      "==> Scraping page 5\n",
      "==> Scraping page 6\n",
      "==> Scraping page 7\n",
      "==> Scraping page 8\n",
      "==> Scraping page 9\n",
      "==> Scraping page 10\n",
      "==> Scraping page 11\n",
      "==> Scraping page 12\n",
      "==> Scraping page 13\n",
      "==> Scraping page 14\n",
      "==> Scraping page 15\n",
      "==> Scraping page 16\n",
      "==> Scraping page 17\n",
      "==> Scraping page 18\n",
      "==> Scraping page 19\n",
      "==> Scraping page 20\n",
      "==> Scraping page 21\n",
      "==> Scraping page 22\n",
      "==> Scraping page 23\n",
      "==> Scraping page 24\n",
      "==> Scraping page 25\n",
      "==> Scraping page 26\n",
      "==> Scraping page 27\n",
      "==> Scraping page 28\n",
      "==> Scraping page 29\n",
      "==> Scraping page 30\n",
      "==> Scraping page 31\n",
      "==> Scraping page 32\n",
      "==> Scraping page 33\n",
      "==> Scraping page 34\n",
      "==> Scraping page 35\n",
      "==> Scraping page 36\n",
      "==> Scraping page 37\n",
      "==> Scraping page 38\n",
      "==> Scraping page 39\n",
      "==> Scraping page 40\n",
      "==> Scraping page 41\n",
      "==> Scraping page 42\n",
      "==> Scraping page 43\n",
      "==> Scraping page 44\n",
      "==> Scraping page 45\n",
      "==> Scraping page 46\n",
      "==> Scraping page 47\n",
      "==> Scraping page 48\n",
      "==> Scraping page 49\n",
      "==> Scraping page 50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "BASE_URL = \"https://www.mql5.com\"\n",
    "START_URL = \"https://www.mql5.com/en/code\"\n",
    "DOWNLOAD_FOLDER = \"mql5_experts\"\n",
    "PAGES_TO_SCRAPE = 50\n",
    "DELAY = 2  # seconds\n",
    "\n",
    "# === SETUP CHROMEDRIVER ===\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(service=Service(), options=chrome_options)\n",
    "\n",
    "# === FONCTION POUR TÉLÉCHARGER UN FICHIER ===\n",
    "def download_file(url, folder_path):\n",
    "    filename = os.path.basename(urlparse(url).path)\n",
    "    filepath = os.path.join(folder_path, filename)\n",
    "    try:\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                for chunk in r.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "            print(f\"[+] Fichier téléchargé : {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Erreur lors du téléchargement : {url} -> {e}\")\n",
    "\n",
    "# === SCRAPER PRINCIPAL ===\n",
    "os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)\n",
    "\n",
    "for page_num in range(PAGES_TO_SCRAPE):\n",
    "    print(f\"==> Scraping page {page_num+1}\")\n",
    "    page_url = f\"{START_URL}/!{page_num}\" if page_num > 0 else START_URL\n",
    "    driver.get(page_url)\n",
    "    time.sleep(DELAY)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    items = soup.select(\".title > a\")\n",
    "\n",
    "    for link in items:\n",
    "        href = link.get(\"href\")\n",
    "        expert_url = urljoin(BASE_URL, href)\n",
    "        driver.get(expert_url)\n",
    "        time.sleep(DELAY)\n",
    "        expert_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Récupérer le nom de l'expert\n",
    "        title_tag = expert_soup.select_one(\"h1\")\n",
    "        if not title_tag:\n",
    "            continue\n",
    "        expert_name = title_tag.text.strip().replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "        expert_folder = os.path.join(DOWNLOAD_FOLDER, expert_name)\n",
    "        os.makedirs(expert_folder, exist_ok=True)\n",
    "        print(f\"[*] Traitement de : {expert_name}\")\n",
    "\n",
    "        # Télécharger le fichier .mq5 ou .mq4\n",
    "        code_links = expert_soup.select(\"a\")\n",
    "        for a in code_links:\n",
    "            href = a.get(\"href\", \"\")\n",
    "            if href.endswith(\".mq5\") or href.endswith(\".mq4\"):\n",
    "                full_url = urljoin(BASE_URL, href)\n",
    "                download_file(full_url, expert_folder)\n",
    "\n",
    "        # Télécharger les images\n",
    "        images = expert_soup.select(\"img\")\n",
    "        for img in images:\n",
    "            src = img.get(\"src\")\n",
    "            if src:\n",
    "                img_url = urljoin(BASE_URL, src)\n",
    "                download_file(img_url, expert_folder)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d87c98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Scraping page 1: https://www.mql5.com/en/code\n",
      "==> Scraping page 2: https://www.mql5.com/en/code/!1\n",
      "==> Scraping page 3: https://www.mql5.com/en/code/!2\n",
      "==> Scraping page 4: https://www.mql5.com/en/code/!3\n",
      "==> Scraping page 5: https://www.mql5.com/en/code/!4\n",
      "==> Scraping page 6: https://www.mql5.com/en/code/!5\n",
      "==> Scraping page 7: https://www.mql5.com/en/code/!6\n",
      "==> Scraping page 8: https://www.mql5.com/en/code/!7\n",
      "==> Scraping page 9: https://www.mql5.com/en/code/!8\n",
      "==> Scraping page 10: https://www.mql5.com/en/code/!9\n",
      "==> Scraping page 11: https://www.mql5.com/en/code/!10\n",
      "==> Scraping page 12: https://www.mql5.com/en/code/!11\n",
      "==> Scraping page 13: https://www.mql5.com/en/code/!12\n",
      "==> Scraping page 14: https://www.mql5.com/en/code/!13\n",
      "==> Scraping page 15: https://www.mql5.com/en/code/!14\n",
      "==> Scraping page 16: https://www.mql5.com/en/code/!15\n",
      "==> Scraping page 17: https://www.mql5.com/en/code/!16\n",
      "==> Scraping page 18: https://www.mql5.com/en/code/!17\n",
      "==> Scraping page 19: https://www.mql5.com/en/code/!18\n",
      "==> Scraping page 20: https://www.mql5.com/en/code/!19\n",
      "==> Scraping page 21: https://www.mql5.com/en/code/!20\n",
      "==> Scraping page 22: https://www.mql5.com/en/code/!21\n",
      "==> Scraping page 23: https://www.mql5.com/en/code/!22\n",
      "==> Scraping page 24: https://www.mql5.com/en/code/!23\n",
      "==> Scraping page 25: https://www.mql5.com/en/code/!24\n",
      "==> Scraping page 26: https://www.mql5.com/en/code/!25\n",
      "==> Scraping page 27: https://www.mql5.com/en/code/!26\n",
      "==> Scraping page 28: https://www.mql5.com/en/code/!27\n",
      "==> Scraping page 29: https://www.mql5.com/en/code/!28\n",
      "==> Scraping page 30: https://www.mql5.com/en/code/!29\n",
      "==> Scraping page 31: https://www.mql5.com/en/code/!30\n",
      "==> Scraping page 32: https://www.mql5.com/en/code/!31\n",
      "==> Scraping page 33: https://www.mql5.com/en/code/!32\n",
      "==> Scraping page 34: https://www.mql5.com/en/code/!33\n",
      "==> Scraping page 35: https://www.mql5.com/en/code/!34\n",
      "==> Scraping page 36: https://www.mql5.com/en/code/!35\n",
      "==> Scraping page 37: https://www.mql5.com/en/code/!36\n",
      "==> Scraping page 38: https://www.mql5.com/en/code/!37\n",
      "==> Scraping page 39: https://www.mql5.com/en/code/!38\n",
      "==> Scraping page 40: https://www.mql5.com/en/code/!39\n",
      "==> Scraping page 41: https://www.mql5.com/en/code/!40\n",
      "==> Scraping page 42: https://www.mql5.com/en/code/!41\n",
      "==> Scraping page 43: https://www.mql5.com/en/code/!42\n",
      "==> Scraping page 44: https://www.mql5.com/en/code/!43\n",
      "==> Scraping page 45: https://www.mql5.com/en/code/!44\n",
      "==> Scraping page 46: https://www.mql5.com/en/code/!45\n",
      "==> Scraping page 47: https://www.mql5.com/en/code/!46\n",
      "==> Scraping page 48: https://www.mql5.com/en/code/!47\n",
      "==> Scraping page 49: https://www.mql5.com/en/code/!48\n",
      "==> Scraping page 50: https://www.mql5.com/en/code/!49\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "BASE_URL = \"https://www.mql5.com\"\n",
    "START_URL = \"https://www.mql5.com/en/code\"\n",
    "DOWNLOAD_FOLDER = \"mql5_experts\"\n",
    "PAGES_TO_SCRAPE = 50\n",
    "DELAY = 2\n",
    "\n",
    "# === CONFIGURATION SELENIUM HEADLESS ===\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(service=Service(), options=chrome_options)\n",
    "\n",
    "# === CRÉER DOSSIER GLOBAL ===\n",
    "os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)\n",
    "\n",
    "# === TÉLÉCHARGER UN FICHIER ===\n",
    "def download_file(url, folder_path):\n",
    "    try:\n",
    "        filename = os.path.basename(urlparse(url).path)\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                for chunk in r.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "            print(f\"[+] Téléchargé : {filename}\")\n",
    "        else:\n",
    "            print(f\"[!] Erreur {r.status_code} pour {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Exception sur téléchargement : {url} -> {e}\")\n",
    "\n",
    "# === SCRAPER PRINCIPAL ===\n",
    "for page_num in range(PAGES_TO_SCRAPE):\n",
    "    page_url = f\"{START_URL}/!{page_num}\" if page_num > 0 else START_URL\n",
    "    print(f\"==> Scraping page {page_num+1}: {page_url}\")\n",
    "    driver.get(page_url)\n",
    "    time.sleep(DELAY)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    links = soup.select(\"div.title a\")\n",
    "\n",
    "    for link in links:\n",
    "        try:\n",
    "            href = link.get(\"href\")\n",
    "            expert_url = urljoin(BASE_URL, href)\n",
    "            driver.get(expert_url)\n",
    "            time.sleep(DELAY)\n",
    "\n",
    "            expert_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            # === NOM DE L'EXPERT ===\n",
    "            title_tag = expert_soup.select_one(\"h1\")\n",
    "            if not title_tag:\n",
    "                continue\n",
    "            expert_name = title_tag.text.strip().replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "            expert_folder = os.path.join(DOWNLOAD_FOLDER, expert_name)\n",
    "            os.makedirs(expert_folder, exist_ok=True)\n",
    "            print(f\"[*] Traitement de : {expert_name}\")\n",
    "\n",
    "            # === LIEN DE TÉLÉCHARGEMENT ZIP OU MQ5 ===\n",
    "            download_link = expert_soup.find(\"a\", string=lambda s: s and \"Download as ZIP\" in s)\n",
    "            if not download_link:\n",
    "                download_link = expert_soup.find(\"a\", href=lambda href: href and (href.endswith(\".mq5\") or href.endswith(\".mq4\")))\n",
    "            if download_link:\n",
    "                file_url = urljoin(BASE_URL, download_link[\"href\"])\n",
    "                download_file(file_url, expert_folder)\n",
    "            else:\n",
    "                print(f\"[!] Aucun fichier à télécharger pour {expert_name}\")\n",
    "\n",
    "            # === TÉLÉCHARGER IMAGES PRÉSENTATION ===\n",
    "            images = expert_soup.select(\"img\")\n",
    "            for img in images:\n",
    "                src = img.get(\"src\")\n",
    "                if src and any(ext in src for ext in ['.png', '.jpg', '.jpeg']):\n",
    "                    img_url = urljoin(BASE_URL, src)\n",
    "                    download_file(img_url, expert_folder)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Erreur sur expert : {link.get('href')} -> {e}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71beeefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5e405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978c290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "9d3732a9",
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "BASE_URL        = \"https://www.mql5.com\"\n",
    "START_PATH      = \"/en/code/mt5/experts\"\n",
    "START_URL       = BASE_URL + START_PATH\n",
    "DOWNLOAD_ROOT   = \"mql5_experts_mt5\"\n",
    "PAGES_TO_SCRAPE = 50      # nombre de pages à parcourir\n",
    "PAGE_DELAY      = 1.0     # délai entre chaque requête détail\n",
    "REQ_TIMEOUT     = 30      # timeout pour requests\n",
    "\n",
    "# === FONCTIONS UTILES ===\n",
    "def safe_mkdir(path):\n",
    "    \"\"\"Crée un dossier s’il n’existe pas.\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def download_file(session, url, dest_folder):\n",
    "    \"\"\"Télécharge un fichier (zip, .mq4/.mq5, image) via requests.\"\"\"\n",
    "    safe_mkdir(dest_folder)\n",
    "    filename = os.path.basename(urlparse(url).path)\n",
    "    dest_path = os.path.join(dest_folder, filename)\n",
    "    print(f\"[↓] {filename} ← {url}\")\n",
    "    try:\n",
    "        resp = session.get(url, stream=True, timeout=REQ_TIMEOUT)\n",
    "        resp.raise_for_status()\n",
    "        with open(dest_path, \"wb\") as f:\n",
    "            for chunk in resp.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "        print(f\"[✔] OK: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[✘] Erreur download {filename}: {e}\")\n",
    "\n",
    "# === PRÉPARATION ===\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
    "safe_mkdir(DOWNLOAD_ROOT)\n",
    "\n",
    "# === SCRAPING ===\n",
    "for page in range(1, PAGES_TO_SCRAPE + 1):\n",
    "    # 1) Charger la page de listing\n",
    "    page_url = START_URL if page == 1 else f\"{START_URL}/page{page}\"\n",
    "    print(f\"\\n=== PAGE {page}/{PAGES_TO_SCRAPE} : {page_url}\")\n",
    "    resp = session.get(page_url, timeout=REQ_TIMEOUT)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # 2) Sélectionner les tuiles d’experts\n",
    "    container = soup.select_one(\"div.codebase-list__content\")\n",
    "    if not container:\n",
    "        print(\"   [!] Conteneur introuvable, on passe.\")\n",
    "        continue\n",
    "\n",
    "    tiles = container.select(\"div.code-tile\")\n",
    "    print(f\"   • {len(tiles)} experts détectés sur cette page\")\n",
    "\n",
    "    for tile in tiles:\n",
    "        # 3) Lien et titre\n",
    "        a = tile.select_one(\"div.title > a\")\n",
    "        if not a or not a.get(\"href\"):\n",
    "            continue\n",
    "        rel_link = a[\"href\"]\n",
    "        # ne garder que /en/code/<id> ou /en/code/mt5/experts/<id>\n",
    "        if not re.match(r\"^/en/code(/mt5/experts)?/\\d+\", rel_link):\n",
    "            continue\n",
    "\n",
    "        title = a.get_text(strip=True)\n",
    "        detail_url = urljoin(BASE_URL, rel_link)\n",
    "        print(f\"\\n→ EA : {title}\")\n",
    "        print(f\"  URL : {detail_url}\")\n",
    "\n",
    "        # Dossier où tout va être téléchargé\n",
    "        safe_name = re.sub(r'[\\\\/:\\*\\?\"<>\\|]', '_', title)\n",
    "        folder = os.path.join(DOWNLOAD_ROOT, safe_name)\n",
    "\n",
    "        # 4) Charger la page détail\n",
    "        dresp = session.get(detail_url, timeout=REQ_TIMEOUT)\n",
    "        dresp.raise_for_status()\n",
    "        dsoup = BeautifulSoup(dresp.text, \"html.parser\")\n",
    "\n",
    "        # 5) Télécharger le ZIP si disponible\n",
    "        zip_link = dsoup.find(\"a\", string=re.compile(r\"Download as ZIP\", re.I))\n",
    "        if zip_link and zip_link.get(\"href\"):\n",
    "            download_file(session, urljoin(BASE_URL, zip_link[\"href\"]), folder)\n",
    "        else:\n",
    "            # sinon le premier .mq5/.mq4\n",
    "            code_link = dsoup.find(\"a\", href=re.compile(r\"\\.mq[45]$\", re.I))\n",
    "            if code_link:\n",
    "                download_file(session, urljoin(BASE_URL, code_link[\"href\"]), folder)\n",
    "            else:\n",
    "                print(\"   [!] Aucun ZIP ni .mq5/.mq4 trouvé\")\n",
    "\n",
    "        # 6) Télécharger toutes les images intégrées\n",
    "        images = dsoup.find_all(\"img\", src=re.compile(r\"\\.(png|jpe?g|gif)$\", re.I))\n",
    "        print(f\"   • {len(images)} images détectées\")\n",
    "        for img in images:\n",
    "            src = img.get(\"src\") or img.get(\"data-src\")\n",
    "            if src:\n",
    "                download_file(session, urljoin(BASE_URL, src), folder)\n",
    "\n",
    "        time.sleep(PAGE_DELAY)\n",
    "\n",
    "print(\"\\n=== TÉLÉCHARGEMENT TERMINÉ ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35fd575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
